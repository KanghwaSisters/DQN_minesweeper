{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqQkFd+/t6/2cnvhiOjPg3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 00 Init"],"metadata":{"id":"rdETq6egdSWb"}},{"cell_type":"markdown","metadata":{"id":"8HOAjQuwL7sN"},"source":["## Mount"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ui3DIJhjL9gg","executionInfo":{"status":"ok","timestamp":1715785055365,"user_tz":-540,"elapsed":17593,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"665354af-8164-4ba7-8a1e-0ba6259c4dc7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Setting to use py files"],"metadata":{"id":"78w_wKa0U7RQ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"0pxUvG0xL7sP","executionInfo":{"status":"ok","timestamp":1715785055365,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"outputs":[],"source":["import os"]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Minesweeper [RL]')"],"metadata":{"id":"bMOGX54HPnN4","executionInfo":{"status":"ok","timestamp":1715785055365,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# check that os is in right directory\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Irw-aDyCMS39","executionInfo":{"status":"ok","timestamp":1715785055365,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"60e630b0-14f8-4aba-b30d-24afb98e7073"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Minesweeper [RL]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["! pip install codes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sr1xfdJ6PsFk","executionInfo":{"status":"ok","timestamp":1715785061690,"user_tz":-540,"elapsed":6330,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"7a013695-051b-450e-f89f-ef6590cdba1f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting codes\n","  Downloading codes-0.1.5-py3-none-any.whl (5.5 kB)\n","Installing collected packages: codes\n","Successfully installed codes-0.1.5\n"]}]},{"cell_type":"markdown","source":["## Import py files"],"metadata":{"id":"tOZz1bXTVUae"}},{"cell_type":"code","source":["# baseline : Env, Agent\n","# from codes.environment.reward5 import *\n","from codes.environment.reward5 import *\n","from codes.agent.vectorDQN import *\n","from codes.net.basic import *\n","from codes.trainer.validShutDown import *\n","from codes.tester.basic import *\n","# import codes.trainer.trainerWithValidShutDown as Trainer\n"],"metadata":{"id":"8QtWl0d3Owjd","executionInfo":{"status":"ok","timestamp":1715785074632,"user_tz":-540,"elapsed":12947,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Import Libraries"],"metadata":{"id":"EQB8jukRfMvE"}},{"cell_type":"markdown","source":["# 01 Info"],"metadata":{"id":"pmz1e_BTfYrZ"}},{"cell_type":"markdown","metadata":{"id":"1p29QQ5GAndP"},"source":["## level dictionary"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Vso5fXO-_hH8","executionInfo":{"status":"ok","timestamp":1715785074632,"user_tz":-540,"elapsed":23,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"outputs":[],"source":["level = {'easy' : {'map_size':(9,9), 'n_mines' : 10},\n","         'medium' : {'map_size':(16,16), 'n_mines':40},\n","         'expert' : {'map_size':(16,30), 'n_mines':99}}"]},{"cell_type":"markdown","source":["## HYPER PARAMETERS"],"metadata":{"id":"M9gDUMM9WBXa"}},{"cell_type":"code","source":["# Environment settings\n","MEM_SIZE = 50000\n","MEM_SIZE_MIN = 1000\n","\n","# Learning settings\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.01\n","LEARN_DECAY = 0.9999975\n","LEARN_MIN = 0.001\n","DISCOUNT = 0.1\n","\n","# Exploration settings\n","EPSILON = 0.95\n","EPSILON_DECAY = 0.99975\n","EPSILON_MIN = 0.01\n","\n","# DQN settings\n","CONV_UNITS = 64\n","UPDATE_TARGET_EVERY = 5"],"metadata":{"id":"ru8shHIbWDZX","executionInfo":{"status":"ok","timestamp":1715785074633,"user_tz":-540,"elapsed":23,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 02 Train, Valid"],"metadata":{"id":"TSbDwBaZgg46"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, env, net, **kwargs):\n","        self.env = env\n","\n","        # Environment Settings\n","        self.mem_size = kwargs.get(\"MEM_SIZE\")\n","        self.mem_size_min = kwargs.get(\"MEM_SIZE_MIN\")\n","\n","        # Learning Settings\n","        self.batch_size = kwargs.get(\"BATCH_SIZE\")\n","        self.learning_rate = kwargs.get(\"LEARNING_RATE\")\n","        self.learn_decay = kwargs.get(\"LEARN_DECAY\")\n","        self.learn_min = kwargs.get(\"LEARN_MIN\")\n","        self.discount = kwargs.get(\"DISCOUNT\")\n","\n","        # Exploration Settings\n","        self.epsilon = kwargs.get(\"EPSILON\")\n","        self.epsilon_decay = kwargs.get(\"EPSILON_DECAY\")\n","        self.epsilon_min = kwargs.get(\"EPSILON_MIN\")\n","\n","        # loss\n","        self.loss_fn = nn.MSELoss()\n","        self.losses = []\n","\n","        # target net update\n","        self.target_update_counter = 0\n","        self.update_target_baseline = kwargs.get(\"UPDATE_TARGET_EVERY\")\n","\n","        # def model\n","        self.model = copy.deepcopy(net)\n","        self.target_model = copy.deepcopy(net)\n","\n","        self.target_model.load_state_dict(self.model.state_dict())\n","\n","        self.model.to(device)\n","        self.target_model.to(device)\n","\n","        # replay memory\n","        self.replay_memory = deque(maxlen=self.mem_size)\n","\n","    def update_target_model(self):\n","        self.target_model.load_state_dict(self.model.state_dict())\n","\n","    def update_replay_memory(self, transition):\n","        self.replay_memory.append(transition)\n","\n","    def get_action(self, state):\n","        '''\n","        get_action은 하나의 state_img만을 받는다.\n","        '''\n","        present_board = state.reshape(self.env.total_tiles) # flatten to get move idx\n","        idx_arr = np.arange(self.env.total_tiles)\n","        unsolved = idx_arr[present_board == self.env.unrevealed]\n","\n","        if np.random.random() < self.epsilon:\n","            # take random action\n","            action = np.random.choice(unsolved)\n","\n","        else:\n","            self.model.eval()\n","\n","            with torch.no_grad():\n","                state = torch.tensor(state.reshape(1,1,self.env.nrows,self.env.ncols),\n","                                         dtype=torch.float32).to(device)\n","                total_action = self.model(state).view(-1)\n","                total_action = total_action.cpu()\n","\n","                # 이미 오픈한 타일은 move 대상에서 제외된다.\n","                total_action[present_board != self.env.unrevealed] = torch.min(total_action)\n","\n","                self.total_action = total_action\n","\n","                action = torch.argmax(total_action).item()\n","\n","        return action\n","\n","    def train(self, done):\n","        if len(self.replay_memory) < self.mem_size_min:\n","            return\n","        # optimizer\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, eps=1e-4)\n","\n","        # 리플레이 메모리에서 배치 사이즈만큼 데이터를 꺼낸다.\n","        # batch[i] = (current_state, action, reward, new_current_state, done)\n","        batch = random.sample(self.replay_memory, self.batch_size)\n","\n","        # 배치 안에 저장되어 있는 정보 꺼내기\n","        current_states, _, _, next_states, _ = zip(*batch)\n","\n","        current_states =  torch.tensor(np.array(current_states), dtype=torch.float32).reshape(-1,1,self.env.nrows,self.env.ncols).to(device)\n","        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).reshape(-1,1,self.env.nrows,self.env.ncols).to(device)\n","\n","        self.model.eval()\n","        self.target_model.eval()\n","\n","        with torch.no_grad():\n","            current_q_values = self.model(current_states).reshape(-1,self.env.total_tiles).cpu().detach().tolist()\n","            next_q_values = self.target_model(next_states).cpu().detach().numpy()\n","\n","        #  current_q_values를 target value가 되도록 업데이트하는 코드\n","        for index, (_, action, reward, _, epi_done) in enumerate(batch):\n","            if not epi_done:\n","                max_future_q = np.max(next_q_values[index])\n","                new_q = reward + self.discount * max_future_q\n","            else:\n","                new_q = reward\n","\n","            current_q_values[index][action] = new_q\n","\n","        # train model\n","        self.model.train()\n","\n","        x = current_states.to(device)\n","        y = torch.tensor(np.array(current_q_values), dtype=torch.float32).to(device)\n","\n","        y_est = self.model(x)\n","\n","        cost = self.loss_fn(y_est, y)\n","\n","        running_loss = cost.item()\n","        self.losses.append(round(running_loss,6))\n","\n","        self.optimizer.zero_grad()\n","        cost.backward()\n","        self.optimizer.step()\n","\n","        if done:\n","            self.target_update_counter += 1\n","\n","        if self.target_update_counter == self.update_target_baseline:\n","            self.update_target_model()\n","            self.target_update_counter = 0\n","\n","        # decay learning rate\n","        self.learning_rate = max(self.learn_min, self.learning_rate*self.learn_decay)\n","\n","        # decay epsilon\n","        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n"],"metadata":{"id":"5pT13XC7vb3a","executionInfo":{"status":"ok","timestamp":1715784762872,"user_tz":-540,"elapsed":527,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class Limited18Agent(Agent):\n","    def __init__(self, env, net, replay_memory=False, **kwargs):\n","        super().__init__(env, net, **kwargs)\n","        print(self.learning_rate)\n","        # 불러올 리플레이 메모리가 있다면 불러옴\n","        if replay_memory:\n","            self.replay_memory = replay_memory\n","\n","    def update_replay_memory(self, transition):\n","        current_state = transition[0]\n","\n","        if np.sum(current_state != self.env.unrevealed) >= 18: # 경험적인 데이터 18(나름 하이퍼파라미터긴 함ㅋ)\n","            self.replay_memory.append(transition)"],"metadata":{"id":"1v1Tb8tvvQ-i","executionInfo":{"status":"ok","timestamp":1715784764970,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["env = MinesweeperEnv(map_size=level['easy']['map_size'],\n","                     n_mines=level['easy']['n_mines'])\n","\n","net = Net(input_dims=env.state.shape,\n","          n_actions=env.total_tiles,\n","          conv_units=CONV_UNITS)\n","\n","agent = Limited18Agent(env=env,\n","                        net=net,\n","                        MEM_SIZE=MEM_SIZE,\n","                        MEM_SIZE_MIN=MEM_SIZE_MIN,\n","                        BATCH_SIZE=BATCH_SIZE,\n","                        LEARNING_RATE=LEARNING_RATE,\n","                        LEARN_DECAY=LEARN_DECAY,\n","                        LEARN_MIN=LEARN_MIN,\n","                        DISCOUNT=DISCOUNT,\n","                        EPSILON=EPSILON,\n","                        EPSILON_DECAY=EPSILON_DECAY,\n","                        EPSILON_MIN=EPSILON_MIN,\n","                        UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY)"],"metadata":{"id":"9YIE1PWMO1Sm","executionInfo":{"status":"ok","timestamp":1715785074633,"user_tz":-540,"elapsed":23,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## TRAIN_PARAMETERS"],"metadata":{"id":"7L5dNbnpiYjH"}},{"cell_type":"code","source":["EPISODES = 200000\n","PRINT_INTERVAL = 1000\n","TRAIN_RENDER = False\n","\n","TRAIN_TIMESTEPS = ['every timestep', 'every episodes']\n","TRAIN_TIMESTEP = TRAIN_TIMESTEPS[0]\n","VIUSAL_INTERVAL = 100\n","\n","VALID_SAMPLE = 1000\n","VALID_INTERVAL = 10"],"metadata":{"id":"6p9LbiVfjFvA","executionInfo":{"status":"ok","timestamp":1715785078351,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["tester_agent = Agent(env=env,\n","                    net=net,\n","                    MEM_SIZE=MEM_SIZE,\n","                    MEM_SIZE_MIN=MEM_SIZE_MIN,\n","                    BATCH_SIZE=BATCH_SIZE,\n","                    LEARNING_RATE=LEARNING_RATE,\n","                    LEARN_DECAY=LEARN_DECAY,\n","                    LEARN_MIN=LEARN_MIN,\n","                    DISCOUNT=DISCOUNT,\n","                    EPSILON=EPSILON,\n","                    EPSILON_DECAY=EPSILON_DECAY,\n","                    EPSILON_MIN=EPSILON_MIN,\n","                    UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY)"],"metadata":{"id":"WSW8m6WyJq_E","executionInfo":{"status":"ok","timestamp":1715785080139,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(env=env,\n","                    agent=agent,\n","                    tester_agent=tester_agent,\n","                    name='episodeInterval',\n","                    train_start=True,\n","                    EPISODES = EPISODES,\n","                    PRINT_INTERVAL = PRINT_INTERVAL,\n","                    TRAIN_RENDER = TRAIN_RENDER,\n","                    TRAIN_TIMESTEP = TRAIN_TIMESTEPS[0],\n","                    VIUSAL_INTERVAL = VIUSAL_INTERVAL,\n","                    VALID_SAMPLE = VALID_SAMPLE,\n","                    VALID_INTERVAL = VALID_INTERVAL)"],"metadata":{"id":"rspyTPvlJk2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 01"],"metadata":{"id":"Jf46VgSZCVYZ"}}]}