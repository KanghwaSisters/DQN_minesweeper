{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmX2qZsxrJ6UF9a+RmjVZf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 00 Init"],"metadata":{"id":"rdETq6egdSWb"}},{"cell_type":"markdown","metadata":{"id":"8HOAjQuwL7sN"},"source":["## Mount"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ui3DIJhjL9gg","executionInfo":{"status":"ok","timestamp":1715772976821,"user_tz":-540,"elapsed":30287,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"a3c52537-e3a0-4a80-d0c1-1ab6c246e1b1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Setting to use py files"],"metadata":{"id":"78w_wKa0U7RQ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"0pxUvG0xL7sP","executionInfo":{"status":"ok","timestamp":1715772976821,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"outputs":[],"source":["import os"]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Minesweeper [RL]')"],"metadata":{"id":"bMOGX54HPnN4","executionInfo":{"status":"ok","timestamp":1715772976821,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# check that os is in right directory\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Irw-aDyCMS39","executionInfo":{"status":"ok","timestamp":1715772976821,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"4c89f807-e9b5-4bc8-8c8b-97fad718dc0d"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Minesweeper [RL]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["! pip install codes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sr1xfdJ6PsFk","executionInfo":{"status":"ok","timestamp":1715772986811,"user_tz":-540,"elapsed":9996,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"b18f7d97-f06a-4796-d5b9-0fd7a4f01bdb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting codes\n","  Downloading codes-0.1.5-py3-none-any.whl (5.5 kB)\n","Installing collected packages: codes\n","Successfully installed codes-0.1.5\n"]}]},{"cell_type":"markdown","source":["## Import py files"],"metadata":{"id":"tOZz1bXTVUae"}},{"cell_type":"code","source":["# baseline : Env, Agent\n","# from codes.environment.reward5 import *\n","from codes.environment.env5reward import *\n","from codes.agent.vectorDQN import *\n","from codes.net.basic import *\n","from codes.trainer.validShutDown import *\n","# import codes.trainer.trainerWithValidShutDown as Trainer\n"],"metadata":{"id":"8QtWl0d3Owjd","executionInfo":{"status":"ok","timestamp":1715773147081,"user_tz":-540,"elapsed":1310,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Import Libraries"],"metadata":{"id":"EQB8jukRfMvE"}},{"cell_type":"markdown","source":["# 01 Info"],"metadata":{"id":"pmz1e_BTfYrZ"}},{"cell_type":"markdown","metadata":{"id":"1p29QQ5GAndP"},"source":["## level dictionary"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Vso5fXO-_hH8","executionInfo":{"status":"ok","timestamp":1715773001766,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"outputs":[],"source":["level = {'easy' : {'map_size':(9,9), 'n_mines' : 10},\n","         'medium' : {'map_size':(16,16), 'n_mines':40},\n","         'expert' : {'map_size':(16,30), 'n_mines':99}}"]},{"cell_type":"markdown","source":["## HYPER PARAMETERS"],"metadata":{"id":"M9gDUMM9WBXa"}},{"cell_type":"code","source":["# Environment settings\n","MEM_SIZE = 50000\n","MEM_SIZE_MIN = 1000\n","\n","# Learning settings\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.01\n","LEARN_DECAY = 0.99975\n","LEARN_MIN = 0.001\n","DISCOUNT = 0.1\n","\n","# Exploration settings\n","EPSILON = 0.95\n","EPSILON_DECAY = 0.99975\n","EPSILON_MIN = 0.01\n","\n","# DQN settings\n","CONV_UNITS = 64\n","UPDATE_TARGET_EVERY = 5"],"metadata":{"id":"ru8shHIbWDZX","executionInfo":{"status":"ok","timestamp":1715773003519,"user_tz":-540,"elapsed":356,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 02 Train, Valid"],"metadata":{"id":"TSbDwBaZgg46"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","class Net(nn.Module):\n","    def __init__(self, input_dims, n_actions, conv_units):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)\n","\n","        self.flatten = nn.Flatten()\n","\n","        fc_size = conv_units * (input_dims[-1]+2) * (input_dims[-2]+2)\n","\n","        self.fc = nn.Linear(fc_size, n_actions)\n","\n","    def forward(self, x):\n","        # conv area\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","\n","        x = self.flatten(x)\n","        # flatten area\n","        x = self.fc(x)\n","\n","        return x"],"metadata":{"id":"1xAFN_Tg45Jg","executionInfo":{"status":"ok","timestamp":1715770473942,"user_tz":-540,"elapsed":514,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import copy\n","from collections import deque\n","from IPython.display import display\n","import pickle\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import random\n","import numpy as np\n","from collections import deque\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MinesweeperEnv:\n","    '''\n","    This env has 5 rewards : win, lose, progress, guess, and no_progress.\n","    '''\n","    def __init__(self,\n","                 map_size,\n","                 n_mines,\n","                 rewards={'win':1, 'lose':-1, 'progress':0.3, 'guess':-0.3, 'no_progress' : -0.3},\n","                 dones={'win':True, 'lose':True, 'progress':False, 'guess':False, 'no_progress' : False}):\n","\n","        # 지뢰찾기 맵에 대한 기본 정보\n","        self.map_size = map_size\n","        self.nrows, self.ncols = map_size\n","        self.total_tiles = self.nrows*self.ncols # n_tiles에서 변경함\n","        self.total_mines = n_mines\n","\n","        # 학습을 위한 정보\n","        self.rewards = rewards\n","        self.dones = dones\n","\n","        # 지뢰찾기 판 생성\n","        self.board = self.make_init_board()\n","\n","        # state 생성\n","        self.state = self.create_state(self.board)\n","\n","        # 상황 판단을 위한 파라미터\n","        self.unrevealed = -1.0 / 8.0\n","\n","    def seed_mines(self):\n","        actual_board = np.zeros(shape=self.total_tiles, dtype='object')\n","\n","        # 지뢰 생성\n","        mine_indices = np.random.choice(self.total_tiles, self.total_mines, replace=False)\n","        actual_board[mine_indices] = \"M\"\n","\n","        # actual board map_size로 복구\n","        actual_board = actual_board.reshape(self.map_size)\n","\n","        return actual_board\n","\n","    def complete_actual_board(self, actual_board):\n","        padded_actual_board = np.pad(actual_board, pad_width=1, mode='constant', constant_values=0)\n","        completed_actual_board = actual_board\n","\n","        for x in range(0, self.nrows):\n","            for y in range(0, self.ncols):\n","                if actual_board[x, y] == \"M\":\n","                    continue\n","                else:\n","                    kernel = padded_actual_board[x:x+3, y:y+3] # padded_actual_board에서의 x,y값은 기존의 +1이라서\n","                    # kernel[1,1] = 0 _ 논리 상으로는 있는게 맞지만 없어도 문제는 안된다. 중앙이 지뢰일 경우가 없기 때문에\n","                    completed_actual_board[x, y] = np.sum(kernel == 'M')\n","\n","        return completed_actual_board\n","\n","    def make_init_board(self):\n","        board = np.ones(shape=(2,self.nrows, self.ncols),dtype='object') # (revealed_or_not, game_board)\n","        actual_board = self.seed_mines()\n","        actual_board = self.complete_actual_board(actual_board)\n","        board[1] = actual_board\n","\n","        return board\n","\n","    def create_state(self, board):\n","        revealed_mask = board[0]\n","        actual_board = copy.deepcopy(board[1])\n","\n","        # trainable한 형태로 변환\n","        actual_board[actual_board == \"M\"] = -2\n","\n","        masked_state = np.ma.masked_array(actual_board,revealed_mask)\n","        masked_state = masked_state.filled(-1) # -1은 unrevealed를 의미한다.\n","\n","        scaled_state = masked_state / 8\n","        scaled_state = scaled_state.astype(np.float16)\n","\n","        return scaled_state\n","\n","    def get_coord(self, action_idx):\n","        # 선택한 action을 더 가시적이게 나타내기 위해\n","\n","        x = action_idx // self.ncols\n","        y = action_idx % self.ncols\n","\n","        return (x, y)\n","\n","    def click(self, action_idx):\n","        # click한 타일을 reveal\n","        clicked_coord = self.get_coord(action_idx)\n","        self.board[0][clicked_coord] = 0\n","        value = self.board[1][clicked_coord]\n","\n","        unrevealed_mask = self.board[0] # revealed : 0, unrevealed : 1\n","        actual_board = self.board[1].reshape(1,self.total_tiles)\n","\n","        # 첫 번째로 선택한 타일은 지뢰가 아니어야 함.\n","        if (value == 'M') & (np.sum(unrevealed_mask == 0) == 1):\n","            safe_tile_indices = np.nonzero(actual_board!='M')[1]\n","            another_move_idx = np.random.choice(safe_tile_indices)\n","            another_move_coord = self.get_coord(another_move_idx)\n","\n","            # 지뢰를 이전한다.\n","            self.board[1][another_move_coord] = 'M'\n","            self.board[1][clicked_coord] = 0 # 초기화 용\n","\n","            # 갱신한 내용을 바탕으로 다시 판을 계산한다.\n","            self.board[1] = self.complete_actual_board(self.board[1])\n","            value = self.board[1][clicked_coord]\n","\n","        # 선택한 타일이 0이라면 주변의 타일이 깨진다.\n","        if value == 0.0:\n","            self.reveal_neighbors(clicked_coord)\n","\n","    def reveal_neighbors(self, coord):\n","        queue = deque([coord])\n","        seen = set([coord])\n","        while queue:\n","            current_coord = queue.popleft()\n","            x,y = current_coord\n","\n","            if self.board[1][x,y] == 0:\n","                for col in range(max(0,y-1), min(y+2, self.ncols)):\n","                    for row in range(max(0,x-1), min(x+2,self.nrows)):\n","                        if (row, col) not in seen:\n","                            seen.add((row, col))\n","                            queue.append((row, col))\n","\n","                            self.board[0][row, col] = 0\n","\n","    def reset(self):\n","        # 지뢰찾기 판 생성\n","        self.board = self.make_init_board()\n","        # state 생성\n","        self.state = self.create_state(self.board)\n","\n","    def step(self, action_idx):\n","        done = False\n","        coord = self.get_coord(action_idx)\n","\n","        current_mask = copy.deepcopy(self.board[0])\n","\n","        # action에 따라 행동을 수행\n","        self.click(action_idx)\n","\n","        # update state\n","        next_state = self.create_state(self.board)\n","        self.state = next_state\n","\n","        # About Reward\n","        if self.board[1][coord] == 'M':\n","            reward = self.rewards['lose']\n","            done = self.dones['lose']\n","\n","        elif np.sum(self.board[0] == 1) == self.total_mines:\n","            reward = self.rewards['win']\n","            done = self.dones['win']\n","\n","        elif current_mask[coord] == 0: # 이미 깐 타일을 눌렀을 때\n","            reward = self.rewards['no_progress']\n","            done = self.dones['no_progress']\n","\n","        else:\n","            padded_unrevealed = np.pad(current_mask, pad_width=1, mode='constant', constant_values=1)\n","\n","            if np.sum(padded_unrevealed[coord[0]:coord[0]+3, coord[1]:coord[1]+3] == 1) == 9: # 아무 정보 없이 누른 타일\n","                reward = self.rewards['guess']\n","                done = self.dones['guess']\n","\n","            else:\n","                reward = self.rewards['progress']\n","                done = self.dones['progress']\n","\n","        return self.state, reward, done\n","\n","    def render(self, state):\n","        # 원래 값으로 복구한 뒤 시각화한다.\n","        state = (state * 8.0).astype(np.int8)\n","        state = state.astype(object)\n","        state[state == -1] = '.'\n","        state[state == -2] = 'M'\n","        state_df = pd.DataFrame(state.reshape((self.map_size)))\n","\n","        display(state_df.style.applymap(self.color_state))\n","        print(\" \")\n","\n","    def color_state(self, value):\n","        if value == '.':\n","            color = 'white'\n","        elif value == 0:\n","            color = 'slategrey'\n","        elif value == 1:\n","            color = 'blue'\n","        elif value == 2:\n","            color = 'green'\n","        elif value == 3:\n","            color = 'red'\n","        elif value == 4:\n","            color = 'midnightblue'\n","        elif value == 5:\n","            color = 'brown'\n","        elif value == 6:\n","            color = 'aquamarine'\n","        elif value == 7:\n","            color = 'black'\n","        elif value == 8:\n","            color = 'silver'\n","        else:\n","            color = 'magenta'\n","\n","        return f'color: {color}'\n","\n","class LimitedMinesweeperEnv(MinesweeperEnv):\n","    def __init__(self, map_size, n_mines, total_boards=None, train=True):\n","        super().__init__(map_size, n_mines)\n","\n","        self.train = train\n","\n","        if total_boards is None:\n","            with open(\"/content/drive/MyDrive/Minesweeper [RL]/dataset/easy1000boards.pkl\",\"rb\") as f:\n","                self.total_boards = pickle.load(f)\n","        else:\n","            self.total_boards = total_boards\n","\n","        self.n_boards = len(self.total_boards)\n","\n","        if train:\n","            self.board = self.total_boards[0]\n","        else:\n","            self.board_iteration = iter(total_boards)\n","            self.board = next(self.board_iteration)\n","\n","    def reset(self):\n","        self.n_clicks = 0\n","        self.n_progress = 0\n","\n","        if self.train:\n","            self.board = random.choice(self.total_boards)\n","            self.board[0] = np.ones(shape=self.map_size) # board가 수정되기 때문에 초기화해줘야 한다.\n","\n","        else:\n","            self.board = next(self.board_iteration)\n","\n","        self.state = self.create_state(self.board)"],"metadata":{"id":"nSgnyEP8IWpZ","executionInfo":{"status":"ok","timestamp":1715770420630,"user_tz":-540,"elapsed":7564,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","import time"],"metadata":{"id":"xR9A5PGD4-Ns","executionInfo":{"status":"ok","timestamp":1715770500344,"user_tz":-540,"elapsed":1,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, env, net, **kwargs):\n","        self.env = env\n","\n","        # Environment Settings\n","        self.mem_size = kwargs.get(\"MEM_SIZE\")\n","        self.mem_size_min = kwargs.get(\"MEM_SIZE_MIN\")\n","        print(self.mem_size_min)\n","\n","        # Learning Settings\n","        self.batch_size = kwargs.get(\"BATCH_SIZE\")\n","        self.learning_rate = kwargs.get(\"LEARNING_RATE\")\n","        self.learn_decay = kwargs.get(\"LEARN_DECAY\")\n","        self.learn_min = kwargs.get(\"LEARN_MIN\")\n","        self.discount = kwargs.get(\"DISCOUNT\")\n","\n","        # Exploration Settings\n","        self.epsilon = kwargs.get(\"EPSILON\")\n","        self.epsilon_decay = kwargs.get(\"EPSILON_DECAY\")\n","        self.epsilon_min = kwargs.get(\"EPSILON_MIN\")\n","\n","        self.update_target_baseline = kwargs.get(\"UPDATE_TARGET_EVERY\")\n","\n","        self.model = net\n","        self.target_model = net\n","\n","        self.target_model.load_state_dict(self.model.state_dict())\n","\n","        self.replay_memory = deque(maxlen=self.mem_size)\n","\n","        self.loss_fn = nn.MSELoss()\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, eps=1e-4)\n","\n","        self.model.to(device)\n","        self.target_model.to(device)\n","\n","        self.target_update_counter = 0\n","\n","        self.losses = []\n","\n","    def update_target_model(self):\n","        self.target_model.load_state_dict(self.model.state_dict())\n","\n","    def update_replay_memory(self, transition):\n","        self.replay_memory.append(transition)\n","\n","    def get_action(self, state):\n","        '''\n","        get_action은 하나의 state_img만을 받는다.\n","        '''\n","        if np.random.random() < self.epsilon:\n","            # take random action\n","            action = np.random.choice(range(self.env.total_tiles))\n","\n","        else:\n","            self.model.eval()\n","\n","            with torch.no_grad():\n","                state = torch.tensor(state.reshape(1,1,self.env.nrows,self.env.ncols),\n","                                     dtype=torch.float32).to(device)\n","                total_action = self.model(state).view(-1)\n","                total_action = total_action.cpu()\n","\n","                self.total_action = total_action\n","\n","                action = torch.argmax(total_action).item()\n","\n","        return action\n","\n","    def train(self, done):\n","        if len(self.replay_memory) < self.mem_size_min:\n","            print(len(self.replay_memory))\n","            print(self.mem_size_min)\n","            return\n","        print('train')\n","        # 리플레이 메모리에서 배치 사이즈만큼 데이터를 꺼낸다.\n","        # batch[i] = (current_state, action, reward, new_current_state, done)\n","        batch = random.sample(self.replay_memory, self.batch_size)\n","\n","        # 배치 안에 저장되어 있는 정보 꺼내기\n","        current_states, _, _, next_states, _ = zip(*batch)\n","\n","\n","        current_states =  torch.tensor(np.array(current_states), dtype=torch.float32).reshape(-1,1,self.env.nrows,self.env.ncols).to(device)\n","        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).reshape(-1,1,self.env.nrows,self.env.ncols).to(device)\n","\n","        self.model.eval()\n","        self.target_model.eval()\n","\n","        with torch.no_grad():\n","            current_q_values = self.model(current_states).reshape(-1,self.env.total_tiles).cpu().detach().tolist()\n","            next_q_values = self.target_model(next_states).cpu().detach().numpy()\n","\n","        #  current_q_values를 target value가 되도록 업데이트하는 코드\n","        for index, (_, action, reward, _, epi_done) in enumerate(batch):\n","            if not epi_done:\n","                max_future_q = np.max(next_q_values[index])\n","                new_q = reward + self.discount * max_future_q\n","            else:\n","                new_q = reward\n","\n","            current_q_values[index][action] = new_q\n","\n","        # train model\n","        self.model.train()\n","\n","        x = current_states.to(device)\n","        y = torch.tensor(np.array(current_q_values), dtype=torch.float32).to(device)\n","\n","        y_est = self.model(x)\n","\n","        cost = self.loss_fn(y_est, y)\n","\n","        running_loss = cost.item()\n","        self.losses.append(round(running_loss,6))\n","\n","        self.optimizer.zero_grad()\n","        cost.backward()\n","        self.optimizer.step()\n","\n","        if done:\n","            self.target_update_counter += 1\n","\n","        if self.target_update_counter > self.update_target_baseline:\n","            self.target_model.load_state_dict(self.model.state_dict())\n","            self.target_update_counter = 0\n","\n","        # decay learning rate\n","        self.learning_rate = max(self.learn_min, self.learning_rate*self.learn_decay)\n","\n","        # decay epsilon\n","        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n","\n","        print(self.epsilon)\n"],"metadata":{"id":"AKq7xz2lJagT","executionInfo":{"status":"ok","timestamp":1715770664660,"user_tz":-540,"elapsed":1,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self, env, agent, tester_agent, name, train_start=True, **kwargs):\n","        self.env = env\n","        self.agent = agent\n","\n","        self.progress_list = []\n","        self.wins_list = []\n","        self.ep_rewards_list = []\n","\n","        self.name = name\n","        self.tester_agent = tester_agent\n","\n","        self.best_model_train = None\n","        self.best_model_valid = None\n","\n","        self.baseline_train = 0\n","        self.baseline_valid = 0\n","\n","        self.simple_valid = 0\n","\n","        # Parameters\n","        self.episodes = kwargs.get(\"EPISODES\")\n","\n","        self.print_interval = kwargs.get(\"PRINT_INTERVAL\")\n","        self.train_render = kwargs.get(\"TRAIN_RENDER\")\n","        self.train_timestep = kwargs.get(\"TRAIN_TIMESTEP\")\n","\n","        self.valid_sample = kwargs.get(\"VALID_SAMPLE\")\n","        self.valid_interval = kwargs.get(\"VALID_INTERVAL\")\n","\n","        self.visual_interval = kwargs.get(\"VIUSAL_INTERVAL\")\n","        self.interval = 500\n","\n","        if train_start:\n","            self.train()\n","            self.visualize_train()\n","            self.save_model()\n","\n","    def train(self):\n","        TRAIN_TIMESTEPS = ['every timestep', 'every episodes']\n","        print(TRAIN_TIMESTEPS)\n","        print(self.train_timestep)\n","        start = time.time()\n","\n","        win_rate = 0\n","        valid_win_rate = 0\n","\n","        for episode in range(self.episodes):\n","            # print(len(self.agent.replay_memory))\n","            self.env.reset()\n","\n","            n_clicks = 0\n","            done = False\n","            episode_reward = 0\n","\n","            while not done:\n","                current_state = self.env.state\n","\n","                action = self.agent.get_action(current_state)\n","\n","                next_state, reward, done = self.env.step(action)\n","\n","                episode_reward += reward\n","\n","                self.agent.update_replay_memory((current_state, action, reward, next_state, done))\n","\n","                if self.train_timestep == TRAIN_TIMESTEPS[0]: # every timestep\n","                    self.agent.train(done)\n","\n","                n_clicks += 1\n","\n","            if self.train_timestep == TRAIN_TIMESTEPS[1]: # every episodes\n","                self.agent.train(done)\n","\n","            if self.train_render:\n","                self.env.render(self.env.state)\n","                print(episode_reward)\n","\n","            if len(self.agent.replay_memory) < self.agent.mem_size_min:\n","                continue\n","\n","            self.progress_list.append(n_clicks)\n","            self.ep_rewards_list.append(episode_reward)\n","            self.wins_list.append(reward == self.env.rewards['win'])\n","\n","            if (episode+1) % self.print_interval == 0:\n","                med_progress = np.median(self.progress_list[-self.print_interval:])\n","                win_rate = np.sum(self.wins_list[-self.print_interval:]) / self.print_interval\n","                med_reward = np.median(self.ep_rewards_list[-self.print_interval:])\n","\n","                print(f\"Episode: [{self.episodes}/{episode+1}], Median progress: {med_progress:.2f}, Median reward: {med_reward:.2f}, Win rate : {win_rate:.2f}, Epsilon: {self.agent.epsilon:.2f}\")\n","\n","                if win_rate > self.baseline_train:\n","                    self.baseline_train = win_rate\n","                    self.best_model_train = self.agent.model.state_dict()\n","\n","                    self.simple_valid = 10\n","\n","            if self.simple_valid > 0:\n","                    valid_state = self.agent.model.state_dict()\n","                    valid_win_rate = self.valid_model(self.env, self.tester_agent, episode, self.valid_sample, valid_state)\n","                    self.simple_valid -= 1\n","\n","            if win_rate > self.baseline_valid:\n","                self.baseline_valid = valid_win_rate\n","                self.best_model_valid = self.agent.model.state_dict()\n","\n","        print(round(time.time() - start, 2))\n","\n","    def valid_model(self, env, agent, episode, epoch, model_state):\n","\n","        progress_list, wins_list, ep_rewards = [], [], []\n","\n","        agent.epsilon = 0.0 # valid에서는 탐험을 꺼준다.\n","\n","        agent.model.load_state_dict(model_state)\n","        agent.target_model.load_state_dict(model_state)\n","\n","        for i in range(epoch):\n","\n","            env.reset()\n","\n","            done = False\n","            n_clicks = 0\n","            episode_reward = 0\n","\n","            while not done:\n","                current_state = env.state\n","\n","                action = agent.get_action(current_state)\n","\n","                next_state, reward, done = env.step(action)\n","\n","                if (current_state == next_state).all(): # 같은 곳을 계속 누르는 상황을 탈출시키는 ShutDown Code\n","                    done = True\n","\n","                episode_reward += reward\n","                n_clicks += 1\n","\n","            progress_list.append(n_clicks)\n","            ep_rewards.append(episode_reward)\n","            wins_list.append(reward == env.rewards['win'])\n","\n","        print(f\"Valid n:{epoch}, Median progress: {np.median(progress_list):.2f}, Median reward: {np.median(ep_rewards):.2f}, Win rate : {np.sum(wins_list)/len(wins_list)}\")\n","\n","        return np.sum(wins_list)/len(wins_list) # 승률을 반환한다.\n","\n","    def visualize_train(self, progress=True, win_rates=True, rewards=True, losses=True):\n","        progresses = []\n","        win_rates = []\n","        rewards = []\n","        losses = []\n","\n","        for start in range(0, len(self.progress_list)-self.visual_interval, self.visual_interval):\n","            progresses.append(sum(self.progress_list[start:start+self.visual_interval]) / self.visual_interval)\n","            win_rates.append(sum(self.wins_list[start:start+self.visual_interval]) / self.visual_interval)\n","            rewards.append(sum(self.ep_rewards_list[start:start+self.visual_interval]) / self.visual_interval)\n","            losses.append(sum(self.agent.losses[start:start+self.visual_interval]) / self.visual_interval)\n","\n","        xticks = np.arange(0, len(self.progress_list), self.interval)\n","\n","        if progress:\n","            if len(progresses) > 50:\n","                plt.xticks(xticks, [str(x) + 'K' for x in xticks // 10])\n","            plt.axhline(y=(sum(self.progress_list)/len(self.progress_list)), color='b', linestyle='-')\n","            plt.scatter(range(len(progresses)), progresses, marker='.',alpha=0.3,\n","                        color=['red' if x == max(progresses) else 'black' for x in progresses])\n","            plt.annotate(max(progresses), (progresses.index(max(progresses))+5, max(progresses)))\n","            plt.title(f\"Median Progress per {self.visual_interval} episodes\")\n","            plt.show()\n","\n","        if win_rates:\n","            if len(progresses) > 50:\n","                plt.xticks(xticks, [str(x) + 'K' for x in xticks // 10])\n","                plt.axhline(y=(sum(self.wins_list)/len(self.wins_list)), color='b', linestyle='-')\n","                plt.axhline(y=(sum(self.wins_list[-100:])/len(self.wins_list[-100:])), color='b', linestyle='--')\n","            plt.fill_between(range(len(win_rates)), min(win_rates), win_rates, alpha=0.7)\n","            plt.scatter(win_rates.index(max(win_rates)), max(win_rates), marker='.', color='r')\n","            plt.annotate(max(win_rates), (win_rates.index(max(win_rates))+5, max(win_rates)))\n","            plt.title(f\"Median Win rate per {self.visual_interval} episodes\")\n","            plt.show()\n","\n","        if rewards:\n","            if len(progresses) > 50:\n","                plt.xticks(xticks, [str(x) + 'K' for x in xticks // 10])\n","                plt.axhline(y=(sum(self.ep_rewards_list)/len(self.ep_rewards_list)), color='b', linestyle='-')\n","            plt.scatter(range(len(rewards)), rewards,\n","                        marker='.', alpha=0.3, color=['red' if x == max(rewards) else 'black' for x in rewards])\n","            plt.annotate(round(max(rewards),2), (rewards.index(max(rewards))+5, max(rewards)))\n","            plt.title(f\"Median Episode Reward per {self.visual_interval} episodes\")\n","            plt.show()\n","\n","        if losses:\n","            if len(progresses) > 50:\n","                plt.xticks(xticks, [str(x) + 'K' for x in xticks // 10])\n","            plt.plot(losses)\n","            plt.title(f\"Median Loss per {self.visual_interval} episodes\")\n","            plt.show()\n","\n","    def save_model(self):\n","\n","        def save_file(direction, fname, file):\n","           with open(os.path.join(direction, f'{fname}.pkl'), 'wb') as f:\n","                pickle.dump(file,f)\n","\n","        def create_file(path, name):\n","            file_path = path + '/' + name\n","            # 파일이 이미 존재하는지 확인\n","            if not os.path.exists(file_path):\n","                os.makedirs(file_path)\n","                print(f\"파일 '{file_path}'가 생성되었습니다.\")\n","            else:\n","                print(f\"파일 '{file_path}'는 이미 존재합니다.\")\n","\n","        save_point = {}\n","        save_point['n_mines'] = self.env.total_mines\n","        save_point['total_episodes'] = len(self.progress_list)\n","        save_point['final_model'] = self.agent.model.state_dict()\n","        save_point['best_model_train'] = self.best_model_train\n","        save_point['best_model_valid'] = self.best_model_valid\n","\n","        self.save_point = save_point\n","\n","        f_path = '/content/drive/MyDrive/Minesweeper [RL]/models'\n","        self.total_path = f_path + '/' + self.name\n","\n","        create_file(f_path, self.name)\n","        save_file(self.total_path, f'{len(self.progress_list)}epi_max_train{self.baseline_train}_valid{self.baseline_valid}',save_point)\n","        print('모델이 저장되었습니다.')"],"metadata":{"id":"U-QI7dS6SM9A","executionInfo":{"status":"ok","timestamp":1715771013950,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["env = MinesweeperEnv(map_size=level['easy']['map_size'],\n","                     n_mines=level['easy']['n_mines'])\n","\n","net = Net(input_dims=env.state.shape,\n","          n_actions=env.total_tiles,\n","          conv_units=CONV_UNITS)\n","\n","agent = Agent(env=env,\n","              net=net,\n","              MEM_SIZE=MEM_SIZE,\n","              MEM_SIZE_MIN=MEM_SIZE_MIN,\n","              BATCH_SIZE=BATCH_SIZE,\n","              LEARNING_RATE=LEARNING_RATE,\n","              LEARN_DECAY=LEARN_DECAY,\n","              LEARN_MIN=LEARN_MIN,\n","              DISCOUNT=DISCOUNT,\n","              EPSILON=EPSILON,\n","              EPSILON_DECAY=EPSILON_DECAY,\n","              EPSILON_MIN=EPSILON_MIN,\n","              UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY)"],"metadata":{"id":"9YIE1PWMO1Sm","executionInfo":{"status":"error","timestamp":1715773301995,"user_tz":-540,"elapsed":421,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"42ef1429-b854-46ba-8f63-88f51faaa5d9"},"execution_count":13,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'Agent' object has no attribute 'learning_rate'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-a8ad00e340cf>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           conv_units=CONV_UNITS)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m agent = Agent(env=env,\n\u001b[0m\u001b[1;32m      9\u001b[0m               \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0mMEM_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMEM_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Minesweeper [RL]/codes/agent/vectorDQN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, net, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'learning_rate'"]}]},{"cell_type":"markdown","source":["## TRAIN_PARAMETERS"],"metadata":{"id":"7L5dNbnpiYjH"}},{"cell_type":"code","source":["EPISODES = 200000\n","PRINT_INTERVAL = 100\n","TRAIN_RENDER = False\n","\n","TRAIN_TIMESTEPS = ['every timestep', 'every episodes']\n","TRAIN_TIMESTEP = TRAIN_TIMESTEPS[0]\n","VIUSAL_INTERVAL = 100\n","\n","VALID_SAMPLE = 1000\n","VALID_INTERVAL = 10"],"metadata":{"id":"6p9LbiVfjFvA","executionInfo":{"status":"ok","timestamp":1715771015679,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["tester_agent = Agent(env=env,\n","                    net=net,\n","                    MEM_SIZE=MEM_SIZE,\n","                    MEM_SIZE_MIN=MEM_SIZE_MIN,\n","                    BATCH_SIZE=BATCH_SIZE,\n","                    LEARNING_RATE=LEARNING_RATE,\n","                    LEARN_DECAY=LEARN_DECAY,\n","                    LEARN_MIN=LEARN_MIN,\n","                    DISCOUNT=DISCOUNT,\n","                    EPSILON=EPSILON,\n","                    EPSILON_DECAY=EPSILON_DECAY,\n","                    EPSILON_MIN=EPSILON_MIN,\n","                    UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY)"],"metadata":{"id":"WSW8m6WyJq_E","executionInfo":{"status":"ok","timestamp":1715771015679,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2393e287-68e0-4874-8f30-11b9adcf24a0"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["1000\n"]}]},{"cell_type":"code","source":["trainer = Trainer(env=env,\n","                    agent=agent,\n","                    tester_agent=tester_agent,\n","                    name='episodeInterval',\n","                    train_start=True,\n","                    EPISODES = EPISODES,\n","                    PRINT_INTERVAL = PRINT_INTERVAL,\n","                    TRAIN_RENDER = TRAIN_RENDER,\n","                    TRAIN_TIMESTEP = TRAIN_TIMESTEPS[0],\n","                    VIUSAL_INTERVAL = VIUSAL_INTERVAL,\n","                    VALID_SAMPLE = VALID_SAMPLE,\n","                    VALID_INTERVAL = VALID_INTERVAL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":998},"id":"rspyTPvlJk2B","executionInfo":{"status":"error","timestamp":1715772861868,"user_tz":-540,"elapsed":5073,"user":{"displayName":"Jimin Lee","userId":"01372747766781604817"}},"outputId":"f06a732f-a18f-421b-8aeb-a9c6ba5e96bc"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["['every timestep', 'every episodes']\n","every timestep\n","train\n","0.9497625\n","train\n","0.949525059375\n","train\n","0.9492876781101562\n","train\n","0.9490503561906287\n","train\n","0.9488130936015811\n","train\n","0.9485758903281807\n","train\n","0.9483387463555987\n","train\n","0.9481016616690098\n","train\n","0.9478646362535925\n","train\n","0.9476276700945292\n","train\n","0.9473907631770055\n","train\n","0.9471539154862113\n","train\n","0.9469171270073398\n","train\n","0.946680397725588\n","train\n","0.9464437276261566\n","train\n","0.9462071166942501\n","train\n","0.9459705649150765\n","train\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-620c0ce84850>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(env=env,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mtester_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtester_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'episodeInterval'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mtrain_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-9454125f6a86>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, agent, tester_agent, name, train_start, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-9454125f6a86>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_timestep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTRAIN_TIMESTEPS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# every timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mn_clicks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-6dcb86094cfd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, done)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_tiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#  current_q_values를 target value가 되도록 업데이트하는 코드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-76095e7fd518>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# 01"],"metadata":{"id":"Jf46VgSZCVYZ"}}]}